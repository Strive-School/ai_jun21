{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "convenient-overview",
   "metadata": {},
   "source": [
    "# Data Augmentation\n",
    "A common strategy for training neural networks is to introduce randomness in the input data itself. For example, you can randomly rotate, mirror, scale, and/or crop your images during training. This will help your network generalize as it's seeing the same images but in different locations, with different sizes, in different orientations, etc.\n",
    "\n",
    "You can import from torchvision the transforms module:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "prospective-south",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "partial-folder",
   "metadata": {},
   "source": [
    "Some of the operation you can do with it:\n",
    "    \n",
    "- Random rotations: `transforms.RandomRotation(degrees)` will perform random rotations of your images \n",
    "    The `degrees` parameter is a range of degrees to select from. If degrees is a number instead of sequence like (min, max), the range of degrees will be (-degrees, +degrees).\n",
    "\n",
    "- Random resize or crop: `torchvision.transforms.RandomResizedCrop(size)`. Crop the given image to random size and aspect ratio. If size is an int, you will get a square image.\n",
    "\n",
    "- Random Horizontal flip: `transforms.RandomHorizontalFlip()` randomly horizontally flip the images.\n",
    "\n",
    "The rest of the available transformations is [here](https://pytorch.org/docs/stable/torchvision/transforms.html) .\n",
    "\n",
    "\n",
    "You can also compose these transformation using:\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "train_transforms = transforms.Compose([transforms.RandomRotation(30),\n",
    "                                       transforms.RandomResizedCrop(224),\n",
    "                                       transforms.RandomHorizontalFlip(),\n",
    "                                       transforms.ToTensor(),\n",
    "                                       transforms.Normalize([0.5, 0.5, 0.5], \n",
    "                                                            [0.5, 0.5, 0.5])])\n",
    "```\n",
    "\n",
    "You'll also typically want to normalize images with `transforms.Normalize`. You pass in a list of means and list of standard deviations, then the color channels are normalized like so\n",
    "\n",
    "```input[channel] = (input[channel] - mean[channel]) / std[channel]```\n",
    "\n",
    "Subtracting `mean` centers the data around zero and dividing by `std` squishes the values to be between -1 and 1. Normalizing helps keep the network work weights near zero which in turn makes backpropagation more stable. *Without normalization, networks will tend to fail to learn.*\n",
    "\n",
    " When you're testing however, you'll want to use images that aren't altered (except you'll need to normalize the same way). So, for validation/test images, you'll typically just resize and crop.\n",
    " \n",
    "Let's try on the MNIST dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "familiar-links",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([transforms.RandomRotation(30),\n",
    "                                       transforms.RandomResizedCrop(16),\n",
    "                                       transforms.RandomHorizontalFlip(),\n",
    "                                       transforms.ToTensor(),\n",
    "                                       transforms.Normalize([0.5], \n",
    "                                                            [0.5])])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "engaged-messenger",
   "metadata": {},
   "source": [
    "Notice that I'm using `transforms.Normalize([0.5], [0.5])])` and not `transforms.Normalize([0.5,0.5,0.5], [0.5,0.5,0.5])])` because in MNIST there are only grayscale images (so only one color channel).\n",
    "\n",
    "Using \n",
    "\n",
    "```datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=True, transform=transform)```\n",
    "\n",
    "\n",
    "the transformation that you have defined must be passed to the `transform` argument as above\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "supposed-repair",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and load the training data\n",
    "trainset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "increased-diagnosis",
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = next(iter(trainloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "green-wound",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 1, 16, 16])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "integral-madrid",
   "metadata": {},
   "source": [
    "As you can see, the shape of the images (last two entries of the cell above) is 16x16 and not 28x28 as expected for the MNIST dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "wanted-torture",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def imshow(image, ax=None, title=None, normalize=True):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "    image = image.numpy().transpose((1, 2, 0))\n",
    "\n",
    "    if normalize:\n",
    "        mean = np.array([0.485, 0.456, 0.406])\n",
    "        std = np.array([0.229, 0.224, 0.225])\n",
    "        image = std * image + mean\n",
    "        image = np.clip(image, 0, 1)\n",
    "\n",
    "    ax.imshow(image)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['left'].set_visible(False)\n",
    "    ax.spines['bottom'].set_visible(False)\n",
    "    ax.tick_params(axis='both', length=0)\n",
    "    ax.set_xticklabels('')\n",
    "    ax.set_yticklabels('')\n",
    "\n",
    "    return ax\n",
    "\n",
    "imshow(images[0], normalize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "selected-ability",
   "metadata": {},
   "source": [
    "Visualizing the image you can see it has been randomly cropped as well!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "filled-stress",
   "metadata": {},
   "source": [
    "You can apply these in your custom dataset as well! Just check the `ImageFolder` method of the `datasets` class in Pytorch:\n",
    "    \n",
    "https://pytorch.org/vision/0.8/datasets.html#imagefolder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "operating-abuse",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
